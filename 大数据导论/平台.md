修改主机名：sudo gedit /etc/hostname
查看IP地址：ifconfig -a
关闭虚拟机防火墙：service iptables stop
查看防火墙状态：service iptables status
配置jdk环境：在profile文件中添加如下：
export	JAVA_HOME=/home/hadoop/java/jdk1.8.0_161
export	JRE_HOME=/home/hadoop/java/jdk1.8.0_161/jre
export	CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH
export	PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$JAVA_HOME:$PATH

export	HADOOP_HOME=/home/hadoop/hadoop-2.7.3
export	PATH="$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH"
export 	HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
检查jdk版本：java -version
更新源列表：sudo apt-get update
安装ssh：sudo apt-get install openssh-server
检查ssh是否启动：sudo ps -e |grep ssh
启动ssh服务：sudo service ssh start
创建ssh-key：ssh-keygen -t rsa --P
将id_rsa.pub追加到authorized_key授权中：cat id_rsa.pub>>authorized_keys
配置hadoop文件：
1、配置core-site.xml:
<property>
   <name>hadoop.tmp.dir</name>
   <value>file:/home/hadoop/hadoop-2.7.3/hdfs/tmp</value>
   <description>A base for other temporary directories.</description>
 </property>
 <property>
  <name>io.file.buffer.size</name>
   <value>131072</value>
 </property>
 <property>
   <name>fs.defaultFS</name>
   <value>hdfs://master:9000</value>
 </property>
2、配置hdfs-site.xml：
<property>
 <name>dfs.replication</name>
   <value>1</value>
 </property>
 <property>
   <name>dfs.namenode.name.dir</name>
   <value>file:/home/hadoop/hadoop-2.7.3/hdfs/name</value>
   <final>true</final>
</property>
 <property>
   <name>dfs.datanode.data.dir</name>
   <value>file:/home/hadoop/hadoop-2.7.3/hdfs/data</value>
   <final>true</final>
 </property>
 <property>
  <name>dfs.namenode.secondary.http-address</name>
   <value>master:9001</value>
 </property>
 <property>
   <name>dfs.webhdfs.enabled</name>
   <value>true</value>
 </property>
 <property>
   <name>dfs.permissions</name>
   <value>false</value>
 </property>
3、mared-site.xml：
<property>
	<name>mepreduce.framework.name</name>
	<value>yarn</value>
</property>
4、yarn-site.xml：
<property>
 <name>yarn.resourcemanager.address</name>
   <value>master:18040</value>
 </property>
 <property>
   <name>yarn.resourcemanager.scheduler.address</name>
   <value>master:18030</value>
 </property>
 <property>
   <name>yarn.resourcemanager.webapp.address</name>
   <value>master:18088</value>
 </property>
 <property>
   <name>yarn.resourcemanager.resource-tracker.address</name>
   <value>master:18025</value>
 </property>
 <property>
   <name>yarn.resourcemanager.admin.address</name>
   <value>master:18141</value>
 </property>
 <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
 </property>
 <property>
     <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
     <value>org.apache.hadoop.mapred.ShuffleHandler</value>
 </property>
配置hadoop环境：
在profile文件后加入：
export HADOOP_HOME=/home/hadoop/hadoop-2.7.3
export PATH="$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH"
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
初始化hadoop：hdfs namenode -format
开启hadoop：
1.启动namenode：hadoop daemon.sh start namenode
2.启动datanode：hadoop daemon.sh start datanode
3.启动其他：start-all.sh